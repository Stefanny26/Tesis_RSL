# ANEXO C: CONJUNTO DE DATOS PARA VALIDACI√ìN DEL GATEKEEPER IA

> **Dataset de Evaluaci√≥n Experimental del Sistema de Validaci√≥n PRISMA**  
> Protocolo de Recolecci√≥n, Etiquetado y An√°lisis

---

## üìã TABLA DE CONTENIDOS

1. [Objetivo del Dataset](#objetivo-del-dataset)
2. [Dise√±o Experimental](#dise√±o-experimental)
3. [Estructura del Dataset](#estructura-del-dataset)
4. [Protocolo de Recolecci√≥n](#protocolo-de-recolecci√≥n)
5. [Protocolo de Etiquetado](#protocolo-de-etiquetado)
6. [Ejecuci√≥n del Experimento](#ejecuci√≥n-del-experimento)
7. [An√°lisis de Resultados](#an√°lisis-de-resultados)
8. [Formato de Archivos](#formato-de-archivos)

---

## 1. OBJETIVO DEL DATASET

### 1.1 Prop√≥sito

Evaluar cuantitativamente la **precisi√≥n del gatekeeper de IA** en validar el cumplimiento de los 27 √≠tems PRISMA 2020, comparando sus decisiones contra un est√°ndar de referencia (gold standard) establecido por evaluadores humanos expertos.

### 1.2 Pregunta de Investigaci√≥n

> **RQ:** ¬øCon qu√© precisi√≥n puede un sistema basado en IA generativa (Gemini 1.5) validar el cumplimiento de los √≠tems PRISMA 2020 en comparaci√≥n con evaluadores humanos expertos?

### 1.3 Hip√≥tesis

- **H1:** El gatekeeper IA alcanzar√° un F1-Score ‚â• 0.80 en la validaci√≥n de √≠tems PRISMA
- **H0:** El gatekeeper IA tendr√° un F1-Score < 0.80 (no suficientemente preciso)

---

## 2. DISE√ëO EXPERIMENTAL

### 2.1 Alcance

**Enfoque:** Evaluar **10 √≠tems cr√≠ticos** en profundidad (en lugar de los 27 superficialmente)

**√çtems Seleccionados:**

| # | √çtem | Secci√≥n | Justificaci√≥n |
|---|------|---------|---------------|
| 1 | T√≠tulo | T√çTULO | Primer filtro, cr√≠tico para identificaci√≥n |
| 2 | Resumen | RESUMEN | Resumen estructurado, m√∫ltiples componentes |
| 5 | Criterios elegibilidad | M√âTODOS | Core metodol√≥gico, reproducibilidad |
| 6 | Fuentes informaci√≥n | M√âTODOS | B√∫squeda exhaustiva, transparencia |
| 7 | Estrategia b√∫squeda | M√âTODOS | T√©cnico, requiere detalle espec√≠fico |
| 16 | Selecci√≥n estudios | RESULTADOS | Diagrama PRISMA, reporte de flujo |
| 17 | Caracter√≠sticas estudios | RESULTADOS | Tabulaci√≥n de datos extra√≠dos |
| 20 | Resultados s√≠ntesis | RESULTADOS | Integraci√≥n de hallazgos |
| 23 | Discusi√≥n | DISCUSI√ìN | Interpretaci√≥n, limitaciones |
| 24 | Registro protocolo | OTRA INFO | Transparencia, pre-registro |

### 2.2 Tama√±o de Muestra

**Por √≠tem:** 200 ejemplos (100 APROBADOS + 100 RECHAZADOS)  
**Total:** 10 √≠tems √ó 200 ejemplos = **2,000 textos**

**Justificaci√≥n estad√≠stica:**
- 200 ejemplos por √≠tem permite detectar diferencias de ¬±5% en accuracy con 95% confianza
- Distribuci√≥n balanceada (50/50) evita sesgo hacia clase mayoritaria

### 2.3 Fuentes de Datos

**Ejemplos APROBADOS (buenos):**
- Revisiones sistem√°ticas publicadas en journals **Q1** (JCR 2022-2024)
- Fuentes:
  - PubMed Central (PMC)
  - Cochrane Library
  - JMIR (Journal of Medical Internet Research)
  - Frontiers in Psychology
- Criterio: RSL que citan expl√≠citamente seguir PRISMA 2020

**Ejemplos RECHAZADOS (malos):**
- 50% de RSL con problemas documentados (revisiones antiguas pre-PRISMA 2020)
- 50% sint√©ticos con errores espec√≠ficos introducidos manualmente

---

## 3. ESTRUCTURA DEL DATASET

### 3.1 Esquema de Datos

```
dataset-validacion-gatekeeper/
‚îú‚îÄ‚îÄ raw/                          # Datos sin procesar
‚îÇ   ‚îú‚îÄ‚îÄ item-01-titulo/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ approved/             # 100 ejemplos buenos
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ejemplo_001.txt
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ejemplo_002.txt
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rejected/             # 100 ejemplos malos
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ejemplo_101.txt
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ item-02-resumen/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ labeled/                      # Etiquetado por expertos
‚îÇ   ‚îú‚îÄ‚îÄ dataset-item-01.csv
‚îÇ   ‚îú‚îÄ‚îÄ dataset-item-02.csv
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ predictions/                  # Resultados del gatekeeper
‚îÇ   ‚îú‚îÄ‚îÄ predictions-item-01.csv
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ analysis/                     # An√°lisis de resultados
‚îÇ   ‚îú‚îÄ‚îÄ confusion-matrices/
‚îÇ   ‚îú‚îÄ‚îÄ metrics-summary.xlsx
‚îÇ   ‚îî‚îÄ‚îÄ error-analysis.md
‚îÇ
‚îî‚îÄ‚îÄ metadata/
    ‚îú‚îÄ‚îÄ sources.csv               # Fuente de cada ejemplo
    ‚îú‚îÄ‚îÄ inter-rater-reliability.csv
    ‚îî‚îÄ‚îÄ experiment-log.md
```

### 3.2 Formato de Archivo CSV

**Archivo:** `labeled/dataset-item-01.csv`

```csv
id,item_number,text,source,label_human,confidence,rater_1,rater_2,notes
001,1,"Aplicaciones de IA en Educaci√≥n: Revisi√≥n Sistem√°tica",PMC_2024_001,APROBADO,HIGH,APROBADO,APROBADO,"Claro y cumple"
002,1,"Estado del Arte de Machine Learning",Synthetic,RECHAZADO,HIGH,RECHAZADO,RECHAZADO,"Falta 'revisi√≥n sistem√°tica'"
003,1,"Revisi√≥n Sistem√°tica del Uso de Realidad Virtual...",Cochrane_2023_045,APROBADO,MEDIUM,APROBADO,NECESITA_MEJORAS,"Rater 2 sugiere m√°s espec√≠fico"
...
```

**Campos:**

| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| `id` | String | Identificador √∫nico (001-200 por √≠tem) |
| `item_number` | Int | N√∫mero de √≠tem PRISMA (1-27) |
| `text` | String | Contenido a evaluar (t√≠tulo, resumen, etc.) |
| `source` | String | Fuente del ejemplo (journal_a√±o_id o "Synthetic") |
| `label_human` | Enum | **APROBADO \| NECESITA_MEJORAS \| RECHAZADO** |
| `confidence` | Enum | HIGH \| MEDIUM \| LOW (confianza en etiqueta) |
| `rater_1` | Enum | Decisi√≥n del evaluador 1 |
| `rater_2` | Enum | Decisi√≥n del evaluador 2 |
| `notes` | String | Observaciones, razones de decisi√≥n |

### 3.3 Archivo de Predicciones

**Archivo:** `predictions/predictions-item-01.csv`

```csv
id,item_number,ai_decision,ai_score,ai_reasoning,ai_issues,ai_suggestions,execution_time_ms,ai_provider,timestamp
001,1,APROBADO,95,"T√≠tulo cumple todos los criterios PRISMA","[]","[]",1523,gemini-1.5-flash,2026-01-15T10:23:45Z
002,1,RECHAZADO,35,"No identifica como revisi√≥n sistem√°tica","[\"Falta palabra clave\"]","[\"Agregar 'Revisi√≥n Sistem√°tica'\"]",1687,gemini-1.5-flash,2026-01-15T10:23:47Z
...
```

---

## 4. PROTOCOLO DE RECOLECCI√ìN

### 4.1 Fase 1: Recolectar Ejemplos APROBADOS (Semanas 1-2)

**Objetivo:** 1,000 ejemplos de RSL de alta calidad

**Pasos:**

1. **B√∫squeda en PubMed Central**
   ```
   Query: "systematic review"[Title] AND "PRISMA 2020"[Text] 
          AND ("2022"[DP] OR "2023"[DP] OR "2024"[DP])
   Filters: Free full text, English/Spanish
   ```

2. **Selecci√≥n de Art√≠culos**
   - Descargar PDF de 100-150 RSL
   - Priorizar: Medicina, Educaci√≥n, Tecnolog√≠a, Psicolog√≠a
   - Verificar que citen PRISMA 2020 en metodolog√≠a

3. **Extracci√≥n Manual por √çtem**
   - Para cada RSL seleccionada:
     - Extraer el t√≠tulo ‚Üí `item-01-titulo/approved/RSL_xxx.txt`
     - Copiar el resumen ‚Üí `item-02-resumen/approved/RSL_xxx.txt`
     - Copiar secci√≥n de criterios ‚Üí `item-05-criterios/approved/RSL_xxx.txt`
     - (Y as√≠ sucesivamente para los 10 √≠tems)

4. **Documentaci√≥n de Fuentes**
   - Registrar en `sources.csv`:
     ```csv
     example_id,item,source_journal,doi,year,quartile
     001,1,JMIR,10.2196/12345,2024,Q1
     ```

### 4.2 Fase 2: Crear Ejemplos RECHAZADOS (Semana 3)

**Estrategia Mixta:**

**A) 50% de RSL Pre-PRISMA 2020 (problemas reales)**

- Buscar RSL publicadas 2010-2019 (antes de PRISMA 2020)
- Extraer secciones que no cumplir√≠an est√°ndar actual
- Com√∫n: t√≠tulos sin "revisi√≥n sistem√°tica", m√©todos vagos

**B) 50% Sint√©ticos (errores controlados)**

Modificar ejemplos APROBADOS introduciendo errores espec√≠ficos:

**Para √çtem 1 (T√≠tulo):**
- Quitar "Revisi√≥n Sistem√°tica" del t√≠tulo
- Hacer t√≠tulo vago ("Aspectos de la IA")
- T√≠tulo demasiado largo (>30 palabras)

**Para √çtem 5 (Criterios):**
- Omitir criterios de exclusi√≥n
- Criterios ambiguos ("estudios relevantes")
- No mencionar idiomas ni fechas

**Para √çtem 16 (Selecci√≥n):**
- No reportar n√∫mero de duplicados
- Omitir estudios excluidos con raz√≥n
- No mencionar diagrama PRISMA

**Documentar errores introducidos:**
```csv
example_id,item,error_type,description
101,1,missing_key_term,"Removed 'systematic review' from title"
102,1,too_vague,"Changed to generic title without specificity"
103,5,incomplete_criteria,"Removed exclusion criteria section"
```

---

## 5. PROTOCOLO DE ETIQUETADO (Gold Standard)

### 5.1 Evaluadores

**Rol: Evaluador 1 (Investigador Principal)**
- Tutor de tesis (experto en metodolog√≠a de investigaci√≥n)
- Familiarizado con PRISMA 2020

**Rol: Evaluador 2 (Investigador Secundario)**
- Investigador con experiencia en RSL
- O las estudiantes (tras capacitaci√≥n en PRISMA)

### 5.2 Proceso de Etiquetado Ciego

**Setup:**
1. Aleatorizar orden de ejemplos (no agrupados por fuente)
2. Cada evaluador trabaja independientemente
3. Sin ver etiquetas del otro evaluador
4. Usar herramienta de etiquetado estandarizada

**Herramienta:** Planilla Excel con macros o app web simple

**Por cada ejemplo:**
1. Leer el texto del √≠tem
2. Revisar criterios PRISMA 2020 del √≠tem (tener gu√≠a a mano)
3. Decidir:
   - ‚úÖ **APROBADO**: Cumple todos los criterios obligatorios
   - ‚ö†Ô∏è **NECESITA_MEJORAS**: Cumple parcialmente o con deficiencias menores
   - ‚ùå **RECHAZADO**: No cumple criterios obligatorios
4. Anotar justificaci√≥n breve
5. Indicar confianza: HIGH / MEDIUM / LOW

### 5.3 Resoluci√≥n de Discrepancias

**C√°lculo de Acuerdo Inter-Evaluador (Kappa de Cohen):**

```python
from sklearn.metrics import cohen_kappa_score

kappa = cohen_kappa_score(rater_1_labels, rater_2_labels)
# Objetivo: Œ∫ > 0.80 (acuerdo casi perfecto)
```

**Proceso de Consenso:**

- Si Œ∫ < 0.75: Revisar gu√≠a de evaluaci√≥n, re-entrenar
- Para casos con desacuerdo:
  1. Reuni√≥n de consenso
  2. Discutir razones de cada evaluador
  3. Re-leer √≠tem PRISMA oficial
  4. Decidir etiqueta final por mayor√≠a (o agregar 3er evaluador)

**Registro:**
```csv
example_id,rater_1,rater_2,final_label,resolution_method
045,APROBADO,NECESITA_MEJORAS,APROBADO,"Consensus after discussion"
```

---

## 6. EJECUCI√ìN DEL EXPERIMENTO

### 6.1 Script de Procesamiento

**Archivo:** `backend/scripts/run-validation-experiment.js`

```javascript
/**
 * Script para ejecutar el experimento de validaci√≥n del gatekeeper
 * 
 * USO:
 *   node scripts/run-validation-experiment.js --item 1 --dataset ./dataset-validacion-gatekeeper/labeled/dataset-item-01.csv
 */

const fs = require('fs');
const csv = require('csv-parser');
const AIService = require('../src/infrastructure/services/ai.service');
const PROMPTS = require('../src/config/prisma-validation-prompts');

async function runExperiment(itemNumber, datasetPath) {
  console.log(`üß™ Iniciando experimento para √çtem ${itemNumber}...`);
  
  const aiService = new AIService();
  const results = [];
  let processed = 0;
  
  // Leer dataset
  const examples = [];
  await new Promise((resolve) => {
    fs.createReadStream(datasetPath)
      .pipe(csv())
      .on('data', (row) => examples.push(row))
      .on('end', resolve);
  });
  
  console.log(`üìä Total de ejemplos: ${examples.length}`);
  
  // Procesar cada ejemplo
  for (const example of examples) {
    try {
      const startTime = Date.now();
      
      // Obtener prompt de validaci√≥n
      const promptConfig = PROMPTS[itemNumber];
      const fullPrompt = promptConfig.validationTemplate.replace(
        '{content}',
        example.text
      );
      
      // Llamar a IA
      const aiResponse = await aiService.generateText(
        promptConfig.systemPrompt,
        fullPrompt,
        'gemini'
      );
      
      // Parsear respuesta
      const validation = JSON.parse(aiResponse);
      
      const executionTime = Date.now() - startTime;
      
      // Registrar resultado
      results.push({
        id: example.id,
        item_number: itemNumber,
        text_preview: example.text.substring(0, 50) + '...',
        label_human: example.label_human,
        ai_decision: validation.decision,
        ai_score: validation.score,
        ai_reasoning: validation.reasoning,
        match: example.label_human === validation.decision,
        execution_time_ms: executionTime,
        timestamp: new Date().toISOString()
      });
      
      processed++;
      if (processed % 10 === 0) {
        console.log(`‚úÖ Procesados: ${processed}/${examples.length}`);
      }
      
      // Rate limiting: esperar 2 segundos entre llamadas
      await new Promise(resolve => setTimeout(resolve, 2000));
      
    } catch (error) {
      console.error(`‚ùå Error procesando ejemplo ${example.id}:`, error.message);
      results.push({
        id: example.id,
        error: error.message
      });
    }
  }
  
  // Guardar resultados
  const outputPath = `./dataset-validacion-gatekeeper/predictions/predictions-item-${itemNumber.toString().padStart(2, '0')}.csv`;
  const csvWriter = require('csv-writer').createObjectCsvWriter({
    path: outputPath,
    header: Object.keys(results[0]).map(key => ({ id: key, title: key }))
  });
  
  await csvWriter.writeRecords(results);
  console.log(`üíæ Resultados guardados en: ${outputPath}`);
  
  // Calcular m√©tricas preliminares
  const matches = results.filter(r => r.match).length;
  const accuracy = (matches / results.length * 100).toFixed(2);
  console.log(`\nüìà ACCURACY PRELIMINAR: ${accuracy}%`);
  
  return results;
}

// Ejecutar
const args = process.argv.slice(2);
const itemNumber = parseInt(args[args.indexOf('--item') + 1]);
const datasetPath = args[args.indexOf('--dataset') + 1];

runExperiment(itemNumber, datasetPath)
  .then(() => console.log('‚úÖ Experimento completado'))
  .catch(err => console.error('‚ùå Error:', err));
```

### 6.2 Ejecuci√≥n Paso a Paso

**D√≠a 1: √çtems 1-2**
```bash
node scripts/run-validation-experiment.js --item 1 --dataset ./dataset/labeled/dataset-item-01.csv
node scripts/run-validation-experiment.js --item 2 --dataset ./dataset/labeled/dataset-item-02.csv
```

**D√≠a 2: √çtems 5-6**
```bash
node scripts/run-validation-experiment.js --item 5 --dataset ./dataset/labeled/dataset-item-05.csv
node scripts/run-validation-experiment.js --item 6 --dataset ./dataset/labeled/dataset-item-06.csv
```

(Y as√≠ sucesivamente...)

**Consideraciones:**
- ‚è±Ô∏è 200 ejemplos √ó 2 seg = ~7 minutos por √≠tem
- üíµ Costo API: ~$0.05-0.10 por √≠tem (Gemini Flash)
- üìä Total experimento: 10 √≠tems √ó 7 min = **70 minutos**

---

## 7. AN√ÅLISIS DE RESULTADOS

### 7.1 Matriz de Confusi√≥n

Para cada √≠tem, calcular:

```
                  Predicci√≥n IA
                APROBADO  RECHAZADO
Humano  APROBADO    TP        FN
        RECHAZADO   FP        TN
```

**Leyenda:**
- **TP (True Positive)**: IA dijo APROBADO, humano tambi√©n ‚Üí ‚úÖ Acierto
- **TN (True Negative)**: IA dijo RECHAZADO, humano tambi√©n ‚Üí ‚úÖ Acierto
- **FP (False Positive)**: IA dijo APROBADO, humano dijo RECHAZADO ‚Üí ‚ùå Error Tipo I (aprueba algo malo)
- **FN (False Negative)**: IA dijo RECHAZADO, humano dijo APROBADO ‚Üí ‚ùå Error Tipo II (rechaza algo bueno)

### 7.2 M√©tricas Calculadas

```python
# Script: analysis/calculate-metrics.py

import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix

def calculate_metrics(predictions_csv, item_number):
    df = pd.read_csv(predictions_csv)
    
    # Mapear a binario: APROBADO=1, RECHAZADO=0
    # (Simplificaci√≥n: NECESITA_MEJORAS ‚Üí 0)
    y_true = df['label_human'].apply(lambda x: 1 if x == 'APROBADO' else 0)
    y_pred = df['ai_decision'].apply(lambda x: 1 if x == 'APROBADO' else 0)
    
    # Calcular m√©tricas
    metrics = {
        'item_number': item_number,
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1_score': f1_score(y_true, y_pred)
    }
    
    # Matriz de confusi√≥n
    cm = confusion_matrix(y_true, y_pred)
    metrics['TP'] = cm[1,1]
    metrics['TN'] = cm[0,0]
    metrics['FP'] = cm[0,1]
    metrics['FN'] = cm[1,0]
    
    return metrics

# Procesar todos los √≠tems
results = []
for i in [1, 2, 5, 6, 7, 16, 17, 20, 23, 24]:
    metrics = calculate_metrics(f'predictions/predictions-item-{i:02d}.csv', i)
    results.append(metrics)
    
# Guardar resultados
df_results = pd.DataFrame(results)
df_results.to_excel('analysis/metrics-summary.xlsx', index=False)
print(df_results)
```

**Output Esperado:**

```
   item  accuracy  precision  recall  f1_score   TP  TN  FP  FN
0     1      0.92       0.94    0.90      0.92   90  94   6  10
1     2      0.88       0.89    0.87      0.88   87  89  11  13
2     5      0.85       0.83    0.88      0.85   88  82  18  12
...
```

### 7.3 An√°lisis Cualitativo de Errores

**Para cada error (FP y FN):**

1. Revisar el texto original
2. Leer razonamiento de la IA
3. Comparar con decisi√≥n humana
4. Categorizar el error:
   - **Ambig√ºedad en texto**: Texto genuinamente dif√≠cil de clasificar
   - **Prompt insuficiente**: IA no capt√≥ criterio espec√≠fico
   - **Error humano**: Evaluador humano se equivoc√≥
   - **Caso l√≠mite**: Frontera entre APROBADO/NECESITA_MEJORAS

**Ejemplo de An√°lisis:**

```markdown
### Error FP #3 (√çtem 1)

**Texto:** "Revisi√≥n de Estrategias de IA en Salud Digital"

**Decisi√≥n IA:** APROBADO (score: 75)
**Razonamiento IA:** "T√≠tulo menciona 'revisi√≥n' y el tema es claro"

**Decisi√≥n Humana:** RECHAZADO
**Justificaci√≥n:** "Falta palabra 'sistem√°tica' expl√≠cita"

**An√°lisis:**
- Error de la IA: No fue suficientemente estricta con el requisito de "revisi√≥n sistem√°tica"
- Acci√≥n: Reforzar en el prompt que "revisi√≥n" solo NO es suficiente
- Categor√≠a: Prompt insuficiente

**Mejora del Prompt:**
Agregar: "CR√çTICO: El t√≠tulo DEBE incluir expl√≠citamente 'Revisi√≥n Sistem√°tica', 
'Systematic Review', o 'Meta-analysis'. Simplemente 'Revisi√≥n' NO es suficiente."
```

---

## 8. FORMATO DE ARCHIVOS

### 8.1 Plantilla de Recolecci√≥n

**Archivo:** `data-collection-template.xlsx`

| ID | √çtem | Texto Completo | Fuente | DOI | A√±o | Notas |
|----|------|----------------|--------|-----|-----|-------|
| 001 | 1 | [T√≠tulo completo] | JMIR | 10.2196/... | 2024 | Q1 journal |

### 8.2 Plantilla de Etiquetado

**Archivo:** `labeling-template.xlsx`

| ID | Texto (primeros 100 chars) | Tu Decisi√≥n | Confianza | Justificaci√≥n |
|----|----------------------------|-------------|-----------|---------------|
| 001 | "Aplicaciones de IA..." | [ ] APROBADO<br>[ ] NECESITA_MEJORAS<br>[ ] RECHAZADO | [ ] HIGH<br>[ ] MEDIUM<br>[ ] LOW | [Explicaci√≥n] |

### 8.3 Reporte Final de Resultados

**Archivo:** `RESULTADOS-EXPERIMENTALES.md`

```markdown
# Resultados del Experimento de Validaci√≥n

## Resumen Ejecutivo

- **Dataset:** 2,000 ejemplos (10 √≠tems √ó 200 ejemplos)
- **Per√≠odo:** 15-25 Enero 2026
- **Modelo IA:** Google Gemini 1.5 Flash
- **Inter-rater Reliability:** Œ∫ = 0.87 (casi perfecto)

## M√©tricas Globales

| M√©trica | Valor | Interpretaci√≥n |
|---------|-------|----------------|
| **Accuracy** | 88.5% | Aciertos generales |
| **Precision** | 87.2% | De lo que aprueba, % correcto |
| **Recall** | 89.8% | De lo bueno real, % detectado |
| **F1-Score** | **0.885** | ‚úÖ Supera objetivo (>0.80) |

## Resultados por √çtem

[Tabla detallada...]

## An√°lisis de Errores

### Falsos Positivos (N=126)
- 45% Ambig√ºedad en texto original
- 30% Prompt puede mejorarse
- 15% Casos l√≠mite APROBADO/NECESITA_MEJORAS
- 10% Error humano

### Falsos Negativos (N=104)
[An√°lisis...]

## Conclusiones

El gatekeeper IA demostr√≥ una precisi√≥n de **F1=0.885**, superando el umbral 
objetivo de 0.80. Esto valida su uso como herramienta de asistencia en la 
validaci√≥n PRISMA, aunque con supervisi√≥n humana recomendada para casos l√≠mite.
```

---

## üìä CRONOGRAMA DE EJECUCI√ìN

| Fase | Actividad | Duraci√≥n | Responsable |
|------|-----------|----------|-------------|
| **Semana 1** | Recolecci√≥n ejemplos APROBADOS (√≠tems 1-5) | 3 d√≠as | Stefanny |
| **Semana 1** | Recolecci√≥n ejemplos APROBADOS (√≠tems 6-10) | 3 d√≠as | Adriana |
| **Semana 2** | Creaci√≥n ejemplos RECHAZADOS (sint√©ticos) | 2 d√≠as | Ambas |
| **Semana 2** | Etiquetado por Evaluador 1 (tutor) | 3 d√≠as | Tutor |
| **Semana 2** | Etiquetado por Evaluador 2 | 3 d√≠as | Estudiante |
| **Semana 3** | Resoluci√≥n de discrepancias | 1 d√≠a | Equipo |
| **Semana 3** | Ejecuci√≥n del experimento (script) | 2 horas | Stefanny |
| **Semana 3** | C√°lculo de m√©tricas | 1 d√≠a | Adriana |
| **Semana 3** | An√°lisis cualitativo de errores | 2 d√≠as | Ambas |
| **Semana 4** | Redacci√≥n de resultados (Cap 4.4) | 3 d√≠as | Ambas |

**Total: 3-4 semanas**

---

## ‚úÖ CHECKLIST DE VALIDACI√ìN

Antes de considerar el dataset completo:

- [ ] 2,000 ejemplos recolectados (10 √≠tems √ó 200)
- [ ] Balance 50/50 (APROBADO/RECHAZADO) en cada √≠tem
- [ ] Fuentes documentadas en `sources.csv`
- [ ] Doble etiquetado ciego completado
- [ ] Kappa de Cohen calculado (Œ∫ > 0.75)
- [ ] Discrepancias resueltas por consenso
- [ ] Script de experimento ejecutado sin errores
- [ ] Predicciones guardadas en CSV
- [ ] Matrices de confusi√≥n calculadas
- [ ] M√©tricas (Precision, Recall, F1) documentadas
- [ ] An√°lisis cualitativo de errores completado
- [ ] Resultados redactados para Cap√≠tulo 4

---

**Contacto para dudas:**
- Stefanny Hern√°ndez: smhernandez2@espe.edu.ec
- Adriana Gonz√°lez: apgonzales1@espe.edu.ec
- Tutor: Paulo Galarza - pcgalarza@espe.edu.ec

**√öltima actualizaci√≥n:** Enero 8, 2026
